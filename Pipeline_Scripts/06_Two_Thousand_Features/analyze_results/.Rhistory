similarityCols <- c("cancer", "seed", "similarity", "set", "acc",
"nmi", "pval", "ci", "se.std", "pvalcox", "coefcox", "con_index_ci",
"con_index_p", "bias_corrected_c_index")
similarity <- loadResults("evaluate_similarity", "similarity.txt", similarityCols)
######################################################################
# Transform pval to -log10(pval)
cluster$actual_pval <- cluster$pval
cluster$pval <- -log10(cluster$pval)
similarity$actual_pval <- similarity$pval
similarity$pval <- -log10(similarity$pval)
# -add "cluster" column, with "SNF" in each entry
# -rename "similarity" column to "impute"
newSimilarity <- similarity
colnames(newSimilarity)[similarityCols=="similarity"] <- "impute"
newSimilarity$cluster <- "SNF"
newSimilarity$impute <- similarityTypes[newSimilarity$impute]
newCluster <- cluster
newCluster$cluster <- clusterTypes[newCluster$cluster]
newCluster$impute <- imputeTypes[newCluster$impute]
combinedScores <- rbind(newCluster, newSimilarity)
combinedScores <- subset(combinedScores, set==1,
c(cancer, impute, cluster, acc, nmi,
pval, ci, se.std, pvalcox, coefcox, con_index_ci, con_index_p, bias_corrected_c_index))
combinedScores$method <- interaction(combinedScores$cluster,
combinedScores$impute, drop=TRUE)
methodTypes <- levels(combinedScores$method)
combinedScores$method <- as.numeric(combinedScores$method)
# Remove Inf pval's
combinedScores$pval[is.infinite(combinedScores$pval)] <- NA
# Write combinedScores to results folder
write.csv(combinedScores, paste0(results_folder, '/scoresTwoThousandDupClust3.csv'))
# Compare if the mean of method 1 is significantly greater than the
# mean of method 2, for every pair of methods and cancer type
xGreaterY <- function(x, y) {
t.test(x, y, alternative="greater", paired=TRUE, na.action = na.omit)$p.value < 0.05
}
testScores <- matrix(, 0, 6)
for (canc in 1:length(cancerTypes)) {
for (met1 in 1:length(methodTypes)) {
score1 <- subset(combinedScores, cancer==canc&method==met1)
accVector <- rep.int(0, length(methodTypes))
nmiVector <- accVector
pvalVector <- accVector
ciVector <- accVector
for (met2 in (1:length(methodTypes))[-met1]) {
score2 <- subset(combinedScores, cancer==canc&method==met2)
accVector[met2] <- xGreaterY(score1$acc, score2$acc)
nmiVector[met2] <- xGreaterY(score1$nmi, score2$nmi)
pvalVector[met2] <- xGreaterY(score1$pval, score2$pval)
ciVector[met2] <- xGreaterY(score1$ci, score2$ci)
}
testScores <- rbind(testScores,
c(canc, met1, sum(accVector), sum(nmiVector),
sum(pvalVector), sum(ciVector)))
}
}
colnames(testScores) <- c("cancer", "method", "acc", "nmi", "pval",
"ci")
testScores <- as.data.frame(testScores)
testScores$cancer <- cancerTypes[testScores$cancer]
testScores$method <- methodTypes[testScores$method]
write.csv(testScores, paste0(results_folder, "/testScores.txt"), row.names = FALSE)
library(ggplot2)
library(reshape2)
library(dplyr)
################################################################################################
# Initialize folders,
home_folder <- "/home/benbrew/hpf/largeprojects/agoldenb/ben"
project_folder <- paste(home_folder, 'Projects/SNF/NM_2015', sep = '/')
scripts_folder <- paste(project_folder, "Scripts", '06_Two_Thousand_Features', sep = "/")
plots_folder <- paste(scripts_folder, "analyze_results/Plots", sep ="/")
results_folder <- paste(project_folder, 'Scripts/06_Results', sep = '/')
source(paste0(results_folder, '/Lib/helpers.R'))
# Load data
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
##### For evaluating and ranking survival metrics for original imputation and similarity
# Scripts/06_Two_Thousand_Features/evaluate_imputation/Results
# Evalute the results of the evaluate_imputation tests
######################################################################
# Load libraries
# library(dplyr)
# library(reshape2)
######################################################################
# Initialize folders
home_folder <- "/home/benbrew/hpf/largeprojects/agoldenb/ben"
project_folder <- paste(home_folder, 'Projects/SNF/NM_2015', sep = '/')
scripts_folder <- paste(project_folder, "Scripts", "Missing_Data", sep = "/")
plots_folder <- paste(scripts_folder, "analyze_results/Plots", sep ="/")
results_folder <- paste(project_folder, 'Scripts/06_Results', sep = '/')
######################################################################
# Initialize fixed variables
cancerTypes <- c("BRCA", "KIRC", "LIHC", "LUAD", "LUSC") # Removed LUSC
dataTypes <- c("methyl", "mirna", "mrna")
imputeTypes <- c("KNN", "LLS", "LSA", "Rand")
clusterTypes <- c("Hierarchical", "iCluster", "SNF")
setTypes <- c("Union", "Intersect")
similarityTypes <- c("Self", "Median", "Regres")
######################################################################
# Load the results
# Accuracy and NMI metrics are meaningless in the original results
# because they were compared to randomly generated labels
loadResults <- function(test_folder, file_name, colNames=NULL) {
file_path <- paste(scripts_folder, test_folder, "Results", file_name,
sep="/")
read.table(file_path, header=FALSE, col.names=colNames)
}
# Load the imputation results
clusterCols <- c("cancer", "impute", "cluster", "set", "acc",
"nmi", "pval", "ci", "se.std","pvalcox", "coefcox", "con_index_ci",
"con_index_p", "bias_corrected_c_index")
imputeCols <- c("cancer", "impute", "seed", "methyl", "mirna", "mrna",
"runtime")
originalCluster <- loadResults("evaluate_original_imputation",
"clustering.txt", clusterCols)
# Load the similarity results
similarityCols <- c("cancer", "similarity", "set", "acc",
"nmi", "pval", "ci", "se.std", "pvalcox", "coefcox", "con_index_ci",
"con_index_p", "bias_corrected_c_index")
originalSimilarity <- loadResults("evaluate_original_similarity",
"similarity.txt",
similarityCols)
######################################################################
# Transform pval to -log10(pval)
originalCluster$pval <- -log10(originalCluster$pval)
originalCluster$pvalcox <- -log10(originalCluster$pvalcox)
originalCluster$con_index_p <- -log10(originalCluster$con_index_p)
originalSimilarity$pval <- -log10(originalSimilarity$pval)
originalSimilarity$pvalcox <- -log10(originalSimilarity$pvalcox)
originalSimilarity$con_index_p <- -log10(originalSimilarity$con_index_p)
############################################################################
# Rank original clustering and similarity resutls
# Combine the similarity and clustering data sets
# Similarity:
# -add "cluster" column, with "SNF" in each entry
# -rename "similarity" column to "impute"
newSimilarity <- originalSimilarity
colnames(newSimilarity)[2] <- "impute"
newSimilarity$cluster <- "SNF"
newSimilarity$impute <- similarityTypes[newSimilarity$impute]
newCluster <- originalCluster
newCluster$cluster <- clusterTypes[newCluster$cluster]
newCluster$impute <- imputeTypes[newCluster$impute]
combinedScores <- rbind(newCluster, newSimilarity)
combinedScores <- subset(combinedScores, set==1,
c(cancer, impute, cluster, acc, nmi,
pval, ci, se.std, pvalcox, coefcox, con_index_ci, con_index_p, bias_corrected_c_index))
combinedScores$method <- interaction(combinedScores$cluster,
combinedScores$impute, drop=TRUE)
methodTypes <- levels(combinedScores$method)
combinedScores$method <- as.numeric(combinedScores$method)
# Remove Inf pval's
combinedScores$pval[is.infinite(combinedScores$pval)] <- NA
write.csv(combinedScores, paste0(results_folder, '/scoresOrigTwoThousandDupClust3.csv'))
# Compare if the mean of method 1 is significantly greater than the
# mean of method 2, for every pair of methods and cancer type
xGreaterY <- function(x, y) {
t.test(x, y, alternative="greater", paired=TRUE, na.action = na.omit)$p.value < 0.05
}
testScores <- matrix(, 0, 6)
for (canc in 1:length(cancerTypes)) {
for (met1 in 1:length(methodTypes)) {
score1 <- subset(combinedScores, cancer==canc&method==met1)
accVector <- rep.int(0, length(methodTypes))
nmiVector <- accVector
pvalVector <- accVector
ciVector <- accVector
for (met2 in (1:length(methodTypes))[-met1]) {
score2 <- subset(combinedScores, cancer==canc&method==met2)
accVector[met2] <- score1$acc > score2$acc
nmiVector[met2] <- score1$nmi > score2$nmi
pvalVector[met2] <-score1$pval > score2$pval
ciVector[met2] <-  score1$ci > score2$ci
}
testScores <- rbind(testScores,
c(canc, met1, sum(accVector), sum(nmiVector),
sum(pvalVector), sum(ciVector)))
}
}
colnames(testScores) <- c("cancer", "method", "acc", "nmi", "pval",
"ci")
testScores <- as.data.frame(testScores)
testScores$cancer <- cancerTypes[testScores$cancer]
testScores$method <- methodTypes[testScores$method]
write.csv(testScores, paste0(results_folder, "/testScoresOrig.csv"), row.names = FALSE)
# Need to evaluate the clustering results
# Scripts/06_Two_Thousand_Features/evaluate_imputation/Results
# Evalute the results of the evaluate_imputation tests
######################################################################
# Load libraries
# library(dplyr)
# library(reshape2)
######################################################################
# Initialize folders
home_folder <- "/home/benbrew/hpf/largeprojects/agoldenb/ben"
project_folder <- paste(home_folder, 'Projects/SNF/NM_2015', sep = '/')
scripts_folder <- paste(project_folder, "Scripts", "Missing_Data", sep = "/")
plots_folder <- paste(scripts_folder, "analyze_results/Plots", sep ="/")
results_folder <- paste(project_folder, 'Scripts/06_Results', sep = '/')
######################################################################
# Initialize fixed variables
cancerTypes <- c("BRCA", "KIRC", "LIHC", "LUAD", "LUSC")
dataTypes <- c("methyl", "mirna", "mrna")
imputeTypes <- c("KNN", "LLS", "LSA", "Rand")
clusterTypes <- c("Hierarchical", "iCluster", "SNF")
setTypes <- c("Union", "Intersect")
similarityTypes <- c("Self", "Median", "Regres")
######################################################################
# Load the results
# Accuracy and NMI metrics are meaningless in the original results
# because they were compared to randomly generated labels
loadResults <- function(test_folder, file_name, colNames=NULL) {
file_path <- paste(scripts_folder, test_folder, "Results", file_name,
sep="/")
read.table(file_path, header=FALSE, col.names=colNames)
}
# Load the imputation results
clusterCols <- c("cancer", "impute", "seed", "cluster", "set", "acc",
"nmi", "pval", "ci", "se.std","pvalcox", "coefcox", "con_index_ci",
"con_index_p", "bias_corrected_c_index")
imputeCols <- c("cancer", "impute", "seed", "methyl", "mirna", "mrna",
"runtime")
cluster <- loadResults("evaluate_imputation", "clustering.txt",
clusterCols)
impute <- loadResults("evaluate_imputation", "imputation.txt",
imputeCols)
# Load the similarity results
similarityCols <- c("cancer", "seed", "similarity", "set", "acc",
"nmi", "pval", "ci", "se.std", "pvalcox", "coefcox", "con_index_ci",
"con_index_p", "bias_corrected_c_index")
similarity <- loadResults("evaluate_similarity", "similarity.txt", similarityCols)
######################################################################
# Transform pval to -log10(pval)
cluster$actual_pval <- cluster$pval
cluster$pval <- -log10(cluster$pval)
similarity$actual_pval <- similarity$pval
similarity$pval <- -log10(similarity$pval)
# -add "cluster" column, with "SNF" in each entry
# -rename "similarity" column to "impute"
newSimilarity <- similarity
colnames(newSimilarity)[similarityCols=="similarity"] <- "impute"
newSimilarity$cluster <- "SNF"
newSimilarity$impute <- similarityTypes[newSimilarity$impute]
newCluster <- cluster
newCluster$cluster <- clusterTypes[newCluster$cluster]
newCluster$impute <- imputeTypes[newCluster$impute]
combinedScores <- rbind(newCluster, newSimilarity)
combinedScores <- subset(combinedScores, set==1,
c(cancer, impute, cluster, acc, nmi,
pval, ci, se.std, pvalcox, coefcox, con_index_ci, con_index_p, bias_corrected_c_index))
combinedScores$method <- interaction(combinedScores$cluster,
combinedScores$impute, drop=TRUE)
methodTypes <- levels(combinedScores$method)
combinedScores$method <- as.numeric(combinedScores$method)
# Remove Inf pval's
combinedScores$pval[is.infinite(combinedScores$pval)] <- NA
# Write combinedScores to results folder
write.csv(combinedScores, paste0(results_folder, '/scoresTwoThousandDupClust3.csv'))
# Compare if the mean of method 1 is significantly greater than the
# mean of method 2, for every pair of methods and cancer type
xGreaterY <- function(x, y) {
t.test(x, y, alternative="greater", paired=TRUE, na.action = na.omit)$p.value < 0.05
}
testScores <- matrix(, 0, 6)
for (canc in 1:length(cancerTypes)) {
for (met1 in 1:length(methodTypes)) {
score1 <- subset(combinedScores, cancer==canc&method==met1)
accVector <- rep.int(0, length(methodTypes))
nmiVector <- accVector
pvalVector <- accVector
ciVector <- accVector
for (met2 in (1:length(methodTypes))[-met1]) {
score2 <- subset(combinedScores, cancer==canc&method==met2)
accVector[met2] <- xGreaterY(score1$acc, score2$acc)
nmiVector[met2] <- xGreaterY(score1$nmi, score2$nmi)
pvalVector[met2] <- xGreaterY(score1$pval, score2$pval)
ciVector[met2] <- xGreaterY(score1$ci, score2$ci)
}
testScores <- rbind(testScores,
c(canc, met1, sum(accVector), sum(nmiVector),
sum(pvalVector), sum(ciVector)))
}
}
colnames(testScores) <- c("cancer", "method", "acc", "nmi", "pval",
"ci")
testScores <- as.data.frame(testScores)
testScores$cancer <- cancerTypes[testScores$cancer]
testScores$method <- methodTypes[testScores$method]
write.csv(testScores, paste0(results_folder, "/testScores.csv"), row.names = FALSE)
library(ggplot2)
library(reshape2)
library(dplyr)
################################################################################################
# Initialize folders,
home_folder <- "/home/benbrew/hpf/largeprojects/agoldenb/ben"
project_folder <- paste(home_folder, 'Projects/SNF/NM_2015', sep = '/')
scripts_folder <- paste(project_folder, "Scripts", '06_Two_Thousand_Features', sep = "/")
plots_folder <- paste(scripts_folder, "analyze_results/Plots", sep ="/")
results_folder <- paste(project_folder, 'Scripts/06_Results', sep = '/')
source(paste0(results_folder, '/Lib/helpers.R'))
# Load data
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
View(testScores)
# get ranking for each method
rankMethods <- function(data, complete) {
if (complete) {
data <- transform(data,
acc_nmi_rank = ave(acc_nmi,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
pval_ci_rank = ave(pval_ci,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
} else {
data <- transform(data,
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
}
return(data)
}
int_rank <- rankMethods(testScores, complete = TRUE)
View(testScores)
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pva_ci <- testScores$pval + testScores$ci
testScoresOrig$acc_nmi <- testScoresOrig$acc + testScoresOrig$nmi
testScoresOrig$pva_ci <- testScoresOrig$pval + testScoresOrig$ci
int_rank <- rankMethods(testScores, complete = TRUE)
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
# add acc and nmi, pval and ci
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScoresOrig$acc_nmi <- testScoresOrig$acc + testScoresOrig$nmi
testScoresOrig$pval_ci <- testScoresOrig$pval + testScoresOrig$ci
# get ranking for each method
rankMethods <- function(data, complete) {
if (complete) {
data <- transform(data,
acc_nmi_rank = ave(acc_nmi,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
pval_ci_rank = ave(pval_ci,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
} else {
data <- transform(data,
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
}
return(data)
}
int_rank <- rankMethods(testScores, complete = TRUE)
testScores$total <- testScores$acc_nmi + testScores$pval_ci
View(testScoresOrig)
testScores$nmi <- NULL
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
# remove unneeded columns
testScores$acc <- NULL
testScores$nmi <- NULL
# add acc and nmi, pval and ci
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScores$total <- testScores$acc_nmi + testScores$pval_ci
testScoresOrig$pval_ci <- testScoresOrig$pval + testScoresOrig$ci
# get ranking for each method
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
testScoresOrig$acc <- NULL
testScoresOrig$nmi <- NULL
# add acc and nmi, pval and ci
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScores$total <- testScores$acc_nmi + testScores$pval_ci
testScoresOrig$pval_ci <- testScoresOrig$pval + testScoresOrig$ci
int_rank <- rankMethods(testScores, complete = TRUE)
union_rank <- rankMethods(testScores, complete = FALSE)
View(int_rank)
View(union_rank)
library(ggplot2)
library(reshape2)
library(dplyr)
################################################################################################
# Initialize folders,
home_folder <- "/home/benbrew/hpf/largeprojects/agoldenb/ben"
project_folder <- paste(home_folder, 'Projects/SNF/NM_2015', sep = '/')
scripts_folder <- paste(project_folder, "Scripts", '06_Two_Thousand_Features', sep = "/")
plots_folder <- paste(scripts_folder, "analyze_results/Plots", sep ="/")
results_folder <- paste(project_folder, 'Scripts/06_Results', sep = '/')
source(paste0(results_folder, '/Lib/helpers.R'))
# Load data
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
# remove unneeded columns
testScoresOrig$acc <- NULL
testScoresOrig$nmi <- NULL
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScores$total <- testScores$acc_nmi + testScores$pval_ci
testScoresOrig$pval_ci <- testScoresOrig$pval + testScoresOrig$ci
View(testScoresOrig)
testScoresOrig$total <- testScoresOrig$pval + testScoresOrig$ci
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
# remove unneeded columns
testScoresOrig$acc <- NULL
testScoresOrig$nmi <- NULL
# add acc and nmi, pval and ci
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScores$total <- testScores$acc_nmi + testScores$pval_ci
testScoresOrig$total <- testScoresOrig$pval + testScoresOrig$ci
# get ranking for each method
# get ranking for each method
rankMethods <- function(data, complete) {
if (complete) {
data <- transform(data,
acc_nmi_rank = ave(acc_nmi,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
pval_ci_rank = ave(pval_ci,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
} else {
data <- transform(data,
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
}
return(data)
}
int_rank <- rankMethods(testScores, complete = TRUE)
union_rank <- rankMethods(testScores, complete = FALSE)
#####
View(union_rank)
View(int_rank)
View(union_rank)
# Load data
testScores <- read.csv(paste0(results_folder, '/testScores.csv'))
testScoresOrig <- read.csv(paste0(results_folder, '/testScoresOrig.csv'))
# remove unneeded columns
testScoresOrig$acc <- NULL
testScoresOrig$nmi <- NULL
# add acc and nmi, pval and ci
testScores$acc_nmi <- testScores$acc + testScores$nmi
testScores$pval_ci <- testScores$pval + testScores$ci
testScores$total <- testScores$acc_nmi + testScores$pval_ci
testScoresOrig$total <- testScoresOrig$pval + testScoresOrig$ci
# get ranking for each method
rankMethods <- function(data, complete) {
if (complete) {
data <- transform(data,
acc_nmi_rank = ave(acc_nmi,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
pval_ci_rank = ave(pval_ci,
cancer,
FUN = function(x) rank(-x, ties.method = "min")),
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
} else {
data <- transform(data,
total_rank = ave(total,
cancer,
FUN = function(x) rank(-x, ties.method = "min")))
}
return(data)
}
int_rank <- rankMethods(testScores, complete = TRUE)
union_rank <- rankMethods(testScoresOrig, complete = FALSE)
View(int_rank)
View(union_rank)
View(int_rank)
int_rank$quintile <- ifelse(int_rank$total_rank > 0 & int_rank$total_rank <= 3, 'top_1',
ifelse(int_rank$total_rank > 3 & int_rank$total_rank <= 6, 'top_2',
ifelse(int_rank$total_rank > 6 & int_rank$total_rank <= 9, 'top_3',
ifelse(int_rank$total_rank > 9 & int_rank$total_rank <= 12, 'top_4', 'top5'))))
View(int_rank)
int_rank$quintile <- ifelse(int_rank$total_rank > 0 & int_rank$total_rank <= 3, 'top_1',
ifelse(int_rank$total_rank > 3 & int_rank$total_rank <= 6, 'top_2',
ifelse(int_rank$total_rank > 6 & int_rank$total_rank <= 9, 'top_3',
ifelse(int_rank$total_rank > 9 & int_rank$total_rank <= 12, 'top_4', 'top_5'))))
View(int_rank)
int_rank$quintile <- ifelse(int_rank$total_rank > 0 & int_rank$total_rank <= 3, 'first',
ifelse(int_rank$total_rank > 3 & int_rank$total_rank <= 6, 'second',
ifelse(int_rank$total_rank > 6 & int_rank$total_rank <= 9, 'third',
ifelse(int_rank$total_rank > 9 & int_rank$total_rank <= 12, '4th', 'fifth'))))
View(int_rank)
int_rank$quintile <- ifelse(int_rank$total_rank > 0 & int_rank$total_rank <= 3, 'first',
ifelse(int_rank$total_rank > 3 & int_rank$total_rank <= 6, 'second',
ifelse(int_rank$total_rank > 6 & int_rank$total_rank <= 9, 'third',
ifelse(int_rank$total_rank > 9 & int_rank$total_rank <= 12, 'fourth', 'fifth'))))
View(int_rank)
union_rank$quunionile <- ifelse(union_rank$total_rank > 0 & union_rank$total_rank <= 3, 'first',
ifelse(union_rank$total_rank > 3 & union_rank$total_rank <= 6, 'second',
ifelse(union_rank$total_rank > 6 & union_rank$total_rank <= 9, 'third',
ifelse(union_rank$total_rank > 9 & union_rank$total_rank <= 12, 'fourth', 'fifth'))))
View(int_rank)
View(union_rank)
